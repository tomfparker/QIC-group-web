---
---

@string{aps = {American Physical Society,}}



@article{angrisani2024classically,
  title={Classically estimating observables of noiseless quantum circuits},
  author={Angrisani, Armando and Schmidhuber, Alexander and Rudolph, Manuel S and Cerezo, M and Holmes, Zo{\"e} and Huang, Hsin-Yuan},
  journal={arXiv preprint arXiv:2409.01706},
  year={2024},
  url = {https://arxiv.org/abs/2409.01706},
  abbr={arXiv preprint}
  
}

@article{xiong2023fundamental,
  title={On fundamental aspects of quantum extreme learning machines},
  author={Xiong, Weijie and Facelli, Giorgio and Sahebi, Mehrad and Agnel, Owen and Chotibut, Thiparat and Thanasilp, Supanut and Holmes, Zo{\"e}},
  journal={arXiv preprint arXiv:2312.15124},
  year={2023},
  html={https://arxiv.org/abs/2312.15124},
  abbr={arXiv preprint}
}

@article{cerezo2023does,
  title={Does provable absence of barren plateaus imply classical simulability? Or, why we need to rethink variational quantum computing},
  author={Cerezo, M and Larocca, Martin and Garc{\'\i}a-Mart{\'\i}n, Diego and Diaz, NL and Braccia, Paolo and Fontana, Enrico and Rudolph, Manuel S and Bermejo, Pablo and Ijaz, Aroosa and Thanasilp, Supanut and others},
  journal={arXiv preprint arXiv:2312.09121},
  year={2023},
  html = {https://arxiv.org/abs/2312.09121},
  abbr={arXiv preprint}
}


@misc{rudolph2023classical,
      title={Classical surrogate simulation of quantum systems with LOWESA}, 
      author={Manuel S. Rudolph and Enrico Fontana and Zoë Holmes and Lukasz Cincio},
      year={2023},
      eprint={2308.09109},
      archivePrefix={arXiv},
      primaryClass={quant-ph},
      month={August},
      publisher={arxiv},
      journal = {arXiv preprint}, 
      html={https://arxiv.org/abs/2308.09109},
      abbr={arXiv preprint}
}


@misc{rudolph2023trainability,
      title={Trainability barriers and opportunities in quantum generative modeling}, 
      author={Manuel S. Rudolph and Sacha Lerch and Supanut Thanasilp and Oriel Kiss and Sofia Vallecorsa and Michele Grossi and Zoë Holmes},
      year={2023},
      eprint={2305.02881},
      archivePrefix={arXiv},
      primaryClass={quant-ph},
      month={May},
      publisher={arxiv},
      journal = {arXiv preprint}, 
      html={https://arxiv.org/abs/2305.02881},
      abbr={arXiv preprint}}

@article{eckstein2023large,
  title={Large-scale simulations of Floquet physics on near-term quantum computers},
  author={Eckstein, Timo and Mansuroglu, Refik and Czarnik, Piotr and Zhu, Jian-Xin and Hartmann, Michael J and Cincio, Lukasz and Sornborger, Andrew T and Holmes, Zo{\"e}},
  journal={arXiv preprint arXiv:2303.02209},
  year={2023},
  month={March},
  html = {https://arxiv.org/abs/2303.02209},
  abbr = {arXiv preprint}
}

@misc{gibbs2021longtime,
      title={Long-time simulations with high fidelity on quantum hardware}, 
      author={Joe Gibbs and Kaitlin Gili and Zoë Holmes and Benjamin Commeau and Andrew Arrasmith and Lukasz Cincio and Patrick J. Coles and Andrew Sornborger},
      year={2022},
      eprint={2102.04313},
      archivePrefix={arXiv},
      primaryClass={quant-ph},
      month={November},
      publisher={Nature},
      journal = {npj}, 
      html={https://www.nature.com/articles/s41534-022-00625-0},
      abbr={npj}}


@article{PhysRev.47.777,
  abbr={Nat. Commun},
  title={Out-of-distribution generalization for learning quantum dynamics},
  author={Caro, Matthias C. and Huang, Hsin-Yuan and Ezzell, Nicholas and Gibbs, Joe and Sornborger, Andrew T. and Cincio, Lukasz and Coles, Patrick J. and Holmes, Zoë},
  journal={NatComms},
  volume={14},
  issue={1},
  year={2023},
  month={May},
  publisher=nature,
  html={https://www.nature.com/articles/s41467-023-39381-w#citeas},


  dimensions={false},
  selected={false}
}


@article{PRXQuantum.3.010313,

  abbr={prx-quantum},
  title = {Connecting Ansatz Expressibility to Gradient Magnitudes and Barren Plateaus},
  author = {Holmes, Zo\"e and Sharma, Kunal and Cerezo, M. and Coles, Patrick J.},
  journal = {PRX Quantum},
  volume = {3},
  issue = {1},
  pages = {010313},
  numpages = {23},
  year = {2022},
  month = {Jan},
  publisher = {American Physical Society}, 
  dimensions={false},
  selected={false},
  html = {https://link.aps.org/doi/10.1103/PRXQuantum.3.010313}
}

@article{Holmes2022quantumalgorithms,
  doi = {10.22331/q-2022-10-06-825},
  url = {https://doi.org/10.22331/q-2022-10-06-825},
  title = {Quantum algorithms from fluctuation theorems: {T}hermal-state preparation},
  author = {Holmes, Zoe and Muraleedharan, Gopikrishnan and Somma, Rolando D. and Subasi, Yigit and {\c{S}}ahino{\u{g}}lu, Burak},
  journal = {{Quantum}},
  issn = {2521-327X},
  publisher = {{Verein zur F{\"{o}}rderung des Open Access Publizierens in den Quantenwissenschaften}},
  volume = {6},
  pages = {825},
  month = {oct},
  year = {2022},
  html = {https://doi.org/10.22331/q-2022-10-06-825},
  abbr = {Quantum}

}


@misc{jerbi2023power,
  title={The power and limitations of learning quantum dynamics incoherently}, 
  author={Sofiene Jerbi and Joe Gibbs and Manuel S. Rudolph and Matthias C. Caro and Patrick J. Coles and Hsin-Yuan Huang and Zoë Holmes},
  year={2023},
  eprint={2303.12834},
  archivePrefix={arXiv},
  primaryClass={quant-ph},
  journal = {arxiv preprint},
  year = {2023},
  month = {May},
  publisher = {arXiv preprint}, 
  dimensions={false},
  selected={false},
  html = {https://arxiv.org/abs/2303.12834},
  abbr = {arXiv preprint}
}

@article{thanasilp2022exponential,
  title={Exponential concentration in quantum kernel methods},
  author={Thanasilp, Supanut and Wang, Samson and Cerezo, M. and Holmes, Zo{\"e}},
  journal={Nature Communications},
  volume={15},
  number={1},
  pages={5200},
  year={2024},
  publisher={Nature Publishing Group UK London},
  url={https://doi.org/10.1038/s41467-024-49287-w},
doi={10.1038/s41467-024-49287-w},
      abbr={Nat. Commun}
}

@misc{gibbs2022dynamical,
      title={Dynamical simulation via quantum machine learning with provable generalization}, 
      author={Joe Gibbs and Zoë Holmes and Matthias C. Caro and Nicholas Ezzell and Hsin-Yuan Huang and Lukasz Cincio and Andrew T. Sornborger and Patrick J. Coles},
      year={2022},
      eprint={2204.10269},
      archivePrefix={arXiv},
      primaryClass={quant-ph},
      month = {April},
      publisher = {arXiv preprint}, 
      dimensions={false},
      selected={false},
      html = {https://arxiv.org/abs/2204.10269},
      abbr = {arXiv preprint}
}


@article{gibbs2025exploitingsymmetries,
	    abstract = {The Lipkin and Agassi models are simplified nuclear models that provide natural test beds for quantum simulation methods. Prior work has investigated the suitability of the variational quantum eigensolver (VQE) to find the ground state of these models. There is a growing awareness that if VQE is to prove viable, we will need problem inspired ans{\"a}tze that take into account the symmetry properties of the problem and use clever initialisation strategies. Here, by focusing on the Lipkin and Agassi models, we investigate how to do this in the context of nuclear physics ground state problems. We further use our observations to discus the potential of new classical, but quantum-inspired, approaches to learning ground states in nuclear problems.},
	    author = {Gibbs, Joe and Holmes, Zo{\"e} and Stevenson, Paul},
	    date = {2025/01/30},
	    date-added = {2025-04-30 14:23:15 +0200},
	    date-modified = {2025-04-30 14:24:37 +0200},
	    doi = {10.1007/s42484-025-00242-y},
	    id = {Gibbs2025},
	    isbn = {2524-4914},
	    journal = {Quantum Machine Intelligence},
	    number = {1},
	    pages = {14},
	    read = {0},
	    title = {Exploiting symmetries in nuclear Hamiltonians for ground state preparation},
	    url = {https://doi.org/10.1007/s42484-025-00242-y},
	    volume = {7},
	    year = {2025},
	    bdsk-url-1 = {https://doi.org/10.1007/s42484-025-00242-y},
      abbr = {QM Intell}
}


@article{weijie2025fundamentalaspects,
	abstract = {Quantum extreme learning machines (QELMs) have emerged as a promising framework for quantum machine learning. Their appeal lies in the rich feature map induced by the dynamics of a quantum substrate---the quantum reservoir---and the efficient post-measurement training via linear regression. Here, we study the expressivity of QELMs by decomposing the prediction of QELMs into a Fourier series. We show that the achievable Fourier frequencies are determined by the data encoding scheme, while Fourier coefficients depend on both the reservoir and the measurement. Notably, the expressivity of QELMs is fundamentally limited by the number of Fourier frequencies and the number of observables, while the complexity of the prediction hinges on the reservoir. As a cautionary note on scalability, we identify four sources that can lead to the exponential concentration of the observables as the system size grows (randomness, hardware noise, entanglement, and global measurements) and show how this can turn QELMs into useless input-agnostic oracles. In particular, our result on the reservoir-induced concentration strongly indicates that quantum reservoirs drawn from a highly random ensemble make QELM models unscalable. Our analysis elucidates the potential and fundamental limitations of QELMs and lays the groundwork for systematically exploring quantum reservoir systems for other machine learning tasks.},
	author = {Xiong, Weijie and Facelli, Giorgio and Sahebi, Mehrad and Agnel, Owen and Chotibut, Thiparat and Thanasilp, Supanut and Holmes, Zo{\"e}},
	date = {2025/02/13},
	date-added = {2025-04-30 14:25:46 +0200},
	date-modified = {2025-04-30 14:26:17 +0200},
	doi = {10.1007/s42484-025-00239-7},
	id = {Xiong2025},
	isbn = {2524-4914},
	journal = {Quantum Machine Intelligence},
	number = {1},
	pages = {20},
	title = {On fundamental aspects of quantum extreme learning machines},
	url = {https://doi.org/10.1007/s42484-025-00239-7},
	volume = {7},
	year = {2025},
	bdsk-url-1 = {https://doi.org/10.1007/s42484-025-00239-7},
      abbr = {QM Intell}
}

@misc{suzuki2025doublebracketalgorithm,
      title={Double-bracket algorithm for quantum signal processing without post-selection}, 
      author={Yudai Suzuki and Bi Hong Tiang and Jeongrak Son and Nelly H. Y. Ng and Zoë Holmes and Marek Gluza},
      year={2025},
      eprint={2504.01077},
      archivePrefix={arXiv},
      primaryClass={quant-ph},
      url={https://arxiv.org/abs/2504.01077}, 
      abbr = {arXiv preprint}
}

@misc{mhiri2025unifyingaccount,
      title={A unifying account of warm start guarantees for patches of quantum landscapes}, 
      author={Hela Mhiri and Ricard Puig and Sacha Lerch and Manuel S. Rudolph and Thiparat Chotibut and Supanut Thanasilp and Zoë Holmes},
      year={2025},
      eprint={2502.07889},
      archivePrefix={arXiv},
      primaryClass={quant-ph},
      url={https://arxiv.org/abs/2502.07889}, 
      abbr = {arXiv preprint}
}

@article{puig2025variationalquantumsimulation,
  title = {Variational Quantum Simulation: A Case Study for Understanding Warm Starts},
  author = {Puig, Ricard and Drudis, Marc and Thanasilp, Supanut and Holmes, Zo\"e},
  journal = {PRX Quantum},
  volume = {6},
  issue = {1},
  pages = {010317},
  numpages = {39},
  year = {2025},
  month = {Jan},
  publisher = {American Physical Society},
  doi = {10.1103/PRXQuantum.6.010317},
  url = {https://link.aps.org/doi/10.1103/PRXQuantum.6.010317},
  abbr = {PRX Quantum}
}

@misc{angrisani2025simulatingquantumcircuits,
      title={Simulating quantum circuits with arbitrary local noise using Pauli Propagation}, 
      author={Armando Angrisani and Antonio A. Mele and Manuel S. Rudolph and M. Cerezo and Zoe Holmes},
      year={2025},
      eprint={2501.13101},
      archivePrefix={arXiv},
      primaryClass={quant-ph},
      url={https://arxiv.org/abs/2501.13101}, 
      abbr = {arXiv preprint}
}

@misc{lerch2024efficientquantumenhancedclassicalsimulation,
      title={Efficient quantum-enhanced classical simulation for patches of quantum landscapes}, 
      author={Sacha Lerch and Ricard Puig and Manuel S. Rudolph and Armando Angrisani and Tyson Jones and M. Cerezo and Supanut Thanasilp and Zoë Holmes},
      year={2024},
      eprint={2411.19896},
      archivePrefix={arXiv},
      primaryClass={quant-ph},
      url={https://arxiv.org/abs/2411.19896}, 
      abbr = {arXiv preprint}
}


@article{rudolph2024trainabilitybarriers,
	abstract = {Quantum generative models provide inherently efficient sampling strategies and thus show promise for achieving an advantage using quantum hardware. In this work, we investigate the barriers to the trainability of quantum generative models posed by barren plateaus and exponential loss concentration. We explore the interplay between explicit and implicit models and losses, and show that using quantum generative models with explicit losses such as the KL divergence leads to a new flavor of barren plateaus. In contrast, the implicit Maximum Mean Discrepancy loss can be viewed as the expectation value of an observable that is either low-bodied and provably trainable, or global and untrainable depending on the choice of kernel. In parallel, we find that solely low-bodied implicit losses cannot in general distinguish high-order correlations in the target data, while some quantum loss estimation strategies can. We validate our findings by comparing different loss functions for modeling data from High-Energy-Physics.},
	author = {Rudolph, Manuel S. and Lerch, Sacha and Thanasilp, Supanut and Kiss, Oriel and Shaya, Oxana and Vallecorsa, Sofia and Grossi, Michele and Holmes, Zo{\"e}},
	date = {2024/11/13},
	date-added = {2025-04-30 14:29:39 +0200},
	date-modified = {2025-04-30 14:30:02 +0200},
	doi = {10.1038/s41534-024-00902-0},
	id = {Rudolph2024},
	isbn = {2056-6387},
	journal = {npj Quantum Information},
	number = {1},
	pages = {116},
	title = {Trainability barriers and opportunities in quantum generative modeling},
	url = {https://doi.org/10.1038/s41534-024-00902-0},
	volume = {10},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1038/s41534-024-00902-0},
  abbr = {npj Quantum Inf}
}


@article{eckstein2024largescalesimulations,
	abstract = {Periodically driven quantum systems exhibit a diverse set of phenomena but are more challenging to simulate than their equilibrium counterparts. Here, we introduce the Quantum High-Frequency Floquet Simulation (QHiFFS) algorithm as a method to simulate fast-driven quantum systems on quantum hardware. Central to QHiFFS is the concept of a kick operator which transforms the system into a basis where the dynamics is governed by a time-independent effective Hamiltonian. This allows prior methods for time-independent simulation to be lifted to simulate Floquet systems. We use the periodically driven biaxial next-nearest neighbor Ising (BNNNI) model, a natural test bed for quantum frustrated magnetism and criticality, as a case study to illustrate our algorithm. We implemented a 20-qubit simulation of the driven two-dimensional BNNNI model on Quantinuum's trapped ion quantum computer. Our error analysis shows that QHiFFS exhibits not only a cubic advantage in driving frequency ωbut also a linear advantage in simulation time t compared to Trotterization.},
	author = {Eckstein, Timo and Mansuroglu, Refik and Czarnik, Piotr and Zhu, Jian-Xin and Hartmann, Michael J. and Cincio, Lukasz and Sornborger, Andrew T. and Holmes, Zo{\"e}},
	date = {2024/09/13},
	date-added = {2025-04-30 14:30:52 +0200},
	date-modified = {2025-04-30 14:31:15 +0200},
	doi = {10.1038/s41534-024-00866-1},
	id = {Eckstein2024},
	isbn = {2056-6387},
	journal = {npj Quantum Information},
	number = {1},
	pages = {84},
	title = {Large-scale simulations of Floquet physics on near-term quantum computers},
	url = {https://doi.org/10.1038/s41534-024-00866-1},
	volume = {10},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1038/s41534-024-00866-1},
  abbr = {npj Quantum Inf}
}

@misc{angrisani2024classicallyestimatingobservables,
      title={Classically estimating observables of noiseless quantum circuits}, 
      author={Armando Angrisani and Alexander Schmidhuber and Manuel S. Rudolph and M. Cerezo and Zoë Holmes and Hsin-Yuan Huang},
      year={2024},
      eprint={2409.01706},
      archivePrefix={arXiv},
      primaryClass={quant-ph},
      url={https://arxiv.org/abs/2409.01706}, 
      abbr = {arXiv preprint}
}


@article{thanasilp2024exponentialconcentration,
	abstract = {Kernel methods in Quantum Machine Learning (QML) have recently gained significant attention as a potential candidate for achieving a quantum advantage in data analysis. Among other attractive properties, when training a kernel-based model one is guaranteed to find the optimal model's parameters due to the convexity of the training landscape. However, this is based on the assumption that the quantum kernel can be efficiently obtained from quantum hardware. In this work we study the performance of quantum kernel models from the perspective of the resources needed to accurately estimate kernel values. We show that, under certain conditions, values of quantum kernels over different input data can be exponentially concentrated (in the number of qubits) towards some fixed value. Thus on training with a polynomial number of measurements, one ends up with a trivial model where the predictions on unseen inputs are independent of the input data. We identify four sources that can lead to concentration including expressivity of data embedding, global measurements, entanglement and noise. For each source, an associated concentration bound of quantum kernels is analytically derived. Lastly, we show that when dealing with classical data, training a parametrized data embedding with a kernel alignment method is also susceptible to exponential concentration. Our results are verified through numerical simulations for several QML tasks. Altogether, we provide guidelines indicating that certain features should be avoided to ensure the efficient evaluation of quantum kernels and so the performance of quantum kernel methods.},
	author = {Thanasilp, Supanut and Wang, Samson and Cerezo, M. and Holmes, Zo{\"e}},
	date = {2024/06/18},
	date-added = {2025-04-30 14:32:16 +0200},
	date-modified = {2025-04-30 14:32:37 +0200},
	doi = {10.1038/s41467-024-49287-w},
	id = {Thanasilp2024},
	isbn = {2041-1723},
	journal = {Nature Communications},
	number = {1},
	pages = {5200},
	title = {Exponential concentration in quantum kernel methods},
	url = {https://doi.org/10.1038/s41467-024-49287-w},
	volume = {15},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1038/s41467-024-49287-w},
  abbr = {Nat. Commun}
}


@article{gibbs2024dynamicalsimulation,
  title = {Dynamical simulation via quantum machine learning with provable generalization},
  author = {Gibbs, Joe and Holmes, Zo\"e and Caro, Matthias C. and Ezzell, Nicholas and Huang, Hsin-Yuan and Cincio, Lukasz and Sornborger, Andrew T. and Coles, Patrick J.},
  journal = {Phys. Rev. Res.},
  volume = {6},
  issue = {1},
  pages = {013241},
  numpages = {17},
  year = {2024},
  month = {Mar},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevResearch.6.013241},
  url = {https://link.aps.org/doi/10.1103/PhysRevResearch.6.013241}, 
  abbr = {Phys. Rev. Res.}
}


@misc{rudolph2023classicalsurrogatesimulation,
      title={Classical surrogate simulation of quantum systems with LOWESA}, 
      author={Manuel S. Rudolph and Enrico Fontana and Zoë Holmes and Lukasz Cincio},
      year={2023},
      eprint={2308.09109},
      archivePrefix={arXiv},
      primaryClass={quant-ph},
      url={https://arxiv.org/abs/2308.09109}, 
      abbr = {arXiv preprint}
}

@article{holmes2023nonlineartransformations,
  title = {Nonlinear transformations in quantum computation},
  author = {Holmes, Zo\"e and Coble, Nolan J. and Sornborger, Andrew T. and Suba\ifmmode \mbox{\c{s}}\else \c{s}\fi{}\ifmmode \imath \else \i \fi{}, Yi\ifmmode \breve{g}\else \u{g}\fi{}it},
  journal = {Phys. Rev. Res.},
  volume = {5},
  issue = {1},
  pages = {013105},
  numpages = {20},
  year = {2023},
  month = {Feb},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevResearch.5.013105},
  url = {https://link.aps.org/doi/10.1103/PhysRevResearch.5.013105},
  abbr = {Phys. Rev. Res.}
}

@article{Holmes2022quantumalgorithms,
  doi = {10.22331/q-2022-10-06-825},
  url = {https://doi.org/10.22331/q-2022-10-06-825},
  title = {Quantum algorithms from fluctuation theorems: {T}hermal-state preparation},
  author = {Holmes, Zoe and Muraleedharan, Gopikrishnan and Somma, Rolando D. and Subasi, Yigit and {\c{S}}ahino{\u{g}}lu, Burak},
  journal = {{Quantum}},
  issn = {2521-327X},
  publisher = {{Verein zur F{\"{o}}rderung des Open Access Publizierens in den Quantenwissenschaften}},
  volume = {6},
  pages = {825},
  month = oct,
  year = {2022},
  abbr = {Quantum}
}

@misc{gibbs2021longtimesimulations,
      title={Long-time simulations with high fidelity on quantum hardware}, 
      author={Joe Gibbs and Kaitlin Gili and Zoë Holmes and Benjamin Commeau and Andrew Arrasmith and Lukasz Cincio and Patrick J. Coles and Andrew Sornborger},
      year={2021},
      eprint={2102.04313},
      archivePrefix={arXiv},
      primaryClass={quant-ph},
      url={https://arxiv.org/abs/2102.04313}, 
      abbr = {arXiv preprint}
}

@article{holmes2022connectingansatzexpressibility,
  title = {Connecting Ansatz Expressibility to Gradient Magnitudes and Barren Plateaus},
  author = {Holmes, Zo\"e and Sharma, Kunal and Cerezo, M. and Coles, Patrick J.},
  journal = {PRX Quantum},
  volume = {3},
  issue = {1},
  pages = {010313},
  numpages = {23},
  year = {2022},
  month = {Jan},
  publisher = {American Physical Society},
  doi = {10.1103/PRXQuantum.3.010313},
  url = {https://link.aps.org/doi/10.1103/PRXQuantum.3.010313},
  abbr = {PRX Quantum}
}

@article{holmes2021barrenplateausprecludelearningscramblers,
  title = {Barren Plateaus Preclude Learning Scramblers},
  author = {Holmes, Zo\"e and Arrasmith, Andrew and Yan, Bin and Coles, Patrick J. and Albrecht, Andreas and Sornborger, Andrew T.},
  journal = {Phys. Rev. Lett.},
  volume = {126},
  issue = {19},
  pages = {190501},
  numpages = {7},
  year = {2021},
  month = {May},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.126.190501},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.126.190501},
  abbr = {Phys. Rev. Lett.}
}

@article{holmes2020gibbsmixing,
  doi = {10.1088/1367-2630/abc602},
  url = {https://dx.doi.org/10.1088/1367-2630/abc602},
  year = {2020},
  month = {nov},
  publisher = {IOP Publishing},
  volume = {22},
  number = {11},
  pages = {113015},
  author = {Holmes, Zoë and Mintert, Florian and Anders, Janet},
  title = {Gibbs mixing of partially distinguishable photons with a polarising beamsplitter membrane},
  journal = {New Journal of Physics},
  abbr = {New J. Phys}
}

@article{holmes2020enhancedenergytransfer,
  title = {Enhanced Energy Transfer to an Optomechanical Piston from Indistinguishable Photons},
  author = {Holmes, Zo\"e and Anders, Janet and Mintert, Florian},
  journal = {Phys. Rev. Lett.},
  volume = {124},
  issue = {21},
  pages = {210601},
  numpages = {6},
  year = {2020},
  month = {May},
  publisher = {American Physical Society},
  doi = {10.1103/PhysRevLett.124.210601},
  url = {https://link.aps.org/doi/10.1103/PhysRevLett.124.210601},
  abbr = {Phys. Rev. Lett.}
}


@Article{holmes2020quantifyingathermality,
  AUTHOR = {Holmes, Zoë and Hinds Mingo, Erick and Chen, Calvin Y.-R. and Mintert, Florian},
  TITLE = {Quantifying Athermality and Quantum Induced Deviations from Classical Fluctuation Relations},
  JOURNAL = {Entropy},
  VOLUME = {22},
  YEAR = {2020},
  NUMBER = {1},
  ARTICLE-NUMBER = {111},
  URL = {https://www.mdpi.com/1099-4300/22/1/111},
  PubMedID = {33285885},
  ISSN = {1099-4300},
  DOI = {10.3390/e22010111},
  abbr = {Entropy}
}

@article{holmes2019coherentfluctuation,
  doi = {10.22331/q-2019-02-25-124},
  url = {https://doi.org/10.22331/q-2019-02-25-124},
  title = {Coherent fluctuation relations: from the abstract to the concrete},
  author = {Holmes, Zo{\"{e}} and Weidt, Sebastian and Jennings, David and Anders, Janet and Mintert, Florian},
  journal = {{Quantum}},
  issn = {2521-327X},
  publisher = {{Verein zur F{\"{o}}rderung des Open Access Publizierens in den Quantenwissenschaften}},
  volume = {3},
  pages = {124},
  month = feb,
  year = {2019},
  abbr = {Quantum}
}


@article{larocca2025barrenplateaus,
	abstract = {Variational quantum computing offers a flexible computational approach with a broad range of applications. However, a key obstacle to realizing their potential is the barren plateau (BP) phenomenon. When a model exhibits a BP, its parameter optimization landscape becomes exponentially flat and featureless as the problem size increases. Importantly, all the moving pieces of an algorithm ---choices of ansatz, initial state, observable, loss function and hardware noise ---can lead to BPs if they are ill-suited. As BPs strongly impact on trainability, researchers have dedicated considerable effort to develop theoretical and heuristic methods to understand and mitigate their effects. As a result, the study of BPs has become a thriving area of research, influencing and exchanging ideas with other fields such as quantum optimal control, tensor networks and learning theory. This article provides a review of the current understanding of the BP phenomenon.},
	author = {Larocca, Mart{\'\i}n and Thanasilp, Supanut and Wang, Samson and Sharma, Kunal and Biamonte, Jacob and Coles, Patrick J. and Cincio, Lukasz and McClean, Jarrod R. and Holmes, Zo{\"e} and Cerezo, M.},
	date = {2025/04/01},
	date-added = {2025-11-05 16:58:06 +0100},
	date-modified = {2025-11-05 16:58:23 +0100},
	doi = {10.1038/s42254-025-00813-9},
	id = {Larocca2025},
	isbn = {2522-5820},
	journal = {Nature Reviews Physics},
	number = {4},
	pages = {174--189},
	title = {Barren plateaus in variational quantum computing},
	url = {https://doi.org/10.1038/s42254-025-00813-9},
	volume = {7},
	year = {2025},
	bdsk-url-1 = {https://doi.org/10.1038/s42254-025-00813-9},
  abbr = {Nat. Rev. Phys.}
  }

@article{xiong2025rolescramblingnoisetemporal,
      title={Role of scrambling and noise in temporal information processing with quantum systems}, 
      author={Weijie Xiong and Zoë Holmes and Armando Angrisani and Yudai Suzuki and Thiparat Chotibut and Supanut Thanasilp},
      year={2025},
      eprint={2505.10080},
      archivePrefix={arXiv},
      primaryClass={quant-ph},
      url={https://arxiv.org/abs/2505.10080}, 
      abbr = {arXiv preprint}
}


@article{sahebi2025dequantizationsupervisedquantummachine,
      title={On Dequantization of Supervised Quantum Machine Learning via Random Fourier Features}, 
      author={Mehrad Sahebi and Alice Barthe and Yudai Suzuki and Zoë Holmes and Michele Grossi},
      year={2025},
      eprint={2505.15902},
      archivePrefix={arXiv},
      primaryClass={quant-ph},
      url={https://arxiv.org/abs/2505.15902}, 
      abbr = {arXiv preprint}
}

@article{rudolph2025paulipropagationcomputationalframework,
      title={Pauli Propagation: A Computational Framework for Simulating Quantum Systems}, 
      author={Manuel S. Rudolph and Tyson Jones and Yanting Teng and Armando Angrisani and Zoë Holmes},
      year={2025},
      eprint={2505.21606},
      archivePrefix={arXiv},
      primaryClass={quant-ph},
      url={https://arxiv.org/abs/2505.21606}, 
      abbr = {arXiv preprint}
}


@article{xiong2025onfundamentalaspects,
	abstract = {Quantum extreme learning machines (QELMs) have emerged as a promising framework for quantum machine learning. Their appeal lies in the rich feature map induced by the dynamics of a quantum substrate---the quantum reservoir---and the efficient post-measurement training via linear regression. Here, we study the expressivity of QELMs by decomposing the prediction of QELMs into a Fourier series. We show that the achievable Fourier frequencies are determined by the data encoding scheme, while Fourier coefficients depend on both the reservoir and the measurement. Notably, the expressivity of QELMs is fundamentally limited by the number of Fourier frequencies and the number of observables, while the complexity of the prediction hinges on the reservoir. As a cautionary note on scalability, we identify four sources that can lead to the exponential concentration of the observables as the system size grows (randomness, hardware noise, entanglement, and global measurements) and show how this can turn QELMs into useless input-agnostic oracles. In particular, our result on the reservoir-induced concentration strongly indicates that quantum reservoirs drawn from a highly random ensemble make QELM models unscalable. Our analysis elucidates the potential and fundamental limitations of QELMs and lays the groundwork for systematically exploring quantum reservoir systems for other machine learning tasks.},
	author = {Xiong, Weijie and Facelli, Giorgio and Sahebi, Mehrad and Agnel, Owen and Chotibut, Thiparat and Thanasilp, Supanut and Holmes, Zo{\"e}},
	date = {2025/02/13},
	date-added = {2025-11-05 17:04:15 +0100},
	date-modified = {2025-11-05 17:04:15 +0100},
	doi = {10.1007/s42484-025-00239-7},
	id = {Xiong2025},
	isbn = {2524-4914},
	journal = {Quantum Machine Intelligence},
	number = {1},
	pages = {20},
	title = {On fundamental aspects of quantum extreme learning machines},
	url = {https://doi.org/10.1007/s42484-025-00239-7},
	volume = {7},
	year = {2025},
	bdsk-url-1 = {https://doi.org/10.1007/s42484-025-00239-7},
  abbr = {QM Intell}
  }



@article{gibbs2025exploitingsymmetries,
	abstract = {The Lipkin and Agassi models are simplified nuclear models that provide natural test beds for quantum simulation methods. Prior work has investigated the suitability of the variational quantum eigensolver (VQE) to find the ground state of these models. There is a growing awareness that if VQE is to prove viable, we will need problem inspired ans{\"a}tze that take into account the symmetry properties of the problem and use clever initialisation strategies. Here, by focusing on the Lipkin and Agassi models, we investigate how to do this in the context of nuclear physics ground state problems. We further use our observations to discus the potential of new classical, but quantum-inspired, approaches to learning ground states in nuclear problems.},
	author = {Gibbs, Joe and Holmes, Zo{\"e} and Stevenson, Paul},
	date = {2025/01/30},
	date-added = {2025-11-05 17:04:55 +0100},
	date-modified = {2025-11-05 17:04:55 +0100},
	doi = {10.1007/s42484-025-00242-y},
	id = {Gibbs2025},
	isbn = {2524-4914},
	journal = {Quantum Machine Intelligence},
	number = {1},
	pages = {14},
	title = {Exploiting symmetries in nuclear Hamiltonians for ground state preparation},
	url = {https://doi.org/10.1007/s42484-025-00242-y},
	volume = {7},
	year = {2025},
	bdsk-url-1 = {https://doi.org/10.1007/s42484-025-00242-y},
  abbr = {QM Intell}
  }


@article{saem2025pitfallstacklingexponentialconcentration,
      title={Pitfalls when tackling the exponential concentration of parameterized quantum models}, 
      author={Reyhaneh Aghaei Saem and Behrang Tafreshi and Zoë Holmes and Supanut Thanasilp},
      year={2025},
      eprint={2507.22054},
      archivePrefix={arXiv},
      primaryClass={quant-ph},
      url={https://arxiv.org/abs/2507.22054}, 
      abbr = {arXiv preprint}
}

@article{suzuki2025groversalgorithmapproximationimaginarytime,
      title={Grover's algorithm is an approximation of imaginary-time evolution}, 
      author={Yudai Suzuki and Marek Gluza and Jeongrak Son and Bi Hong Tiang and Nelly H. Y. Ng and Zoë Holmes},
      year={2025},
      eprint={2507.15065},
      archivePrefix={arXiv},
      primaryClass={quant-ph},
      url={https://arxiv.org/abs/2507.15065}, 
      abbr = {arXiv preprint}
}

@article{chai2025resourceefficientsimulationsparticlescattering,
      title={Resource-Efficient Simulations of Particle Scattering on a Digital Quantum Computer}, 
      author={Yahui Chai and Joe Gibbs and Vincent R. Pascuzzi and Zoë Holmes and Stefan Kühn and Francesco Tacchino and Ivano Tavernelli},
      year={2025},
      eprint={2507.17832},
      archivePrefix={arXiv},
      primaryClass={quant-ph},
      url={https://arxiv.org/abs/2507.17832},
      abbr = {arXiv preprint} 
}

@article{shaya2025complexityquantumstatescircuits,
      title={On the Complexity of Quantum States and Circuits from the Orthogonal and Symplectic Groups}, 
      author={Oxana Shaya and Zoë Holmes and Christoph Hirche and Armando Angrisani},
      year={2025},
      eprint={2509.07573},
      archivePrefix={arXiv},
      primaryClass={quant-ph},
      url={https://arxiv.org/abs/2509.07573}, 
      abbr = {arXiv preprint}
}

@article{kyriienko2025advantagediscretevariationalquantum,
      title={Advantage for Discrete Variational Quantum Algorithms in Circuit Recompilation}, 
      author={Oleksandr Kyriienko and Chukwudubem Umeano and Zoë Holmes},
      year={2025},
      eprint={2510.01154},
      archivePrefix={arXiv},
      primaryClass={quant-ph},
      url={https://arxiv.org/abs/2510.01154}, 
      abbr = {arXiv preprint}
}


@article{upreti2025quantumresourcesbackfirenongaussianity,
      title={When quantum resources backfire: Non-gaussianity and symplectic coherence in noisy bosonic circuits}, 
      author={Varun Upreti and Ulysse Chabaud and Zoë Holmes and Armando Angrisani},
      year={2025},
      eprint={2510.07264},
      archivePrefix={arXiv},
      primaryClass={quant-ph},
      url={https://arxiv.org/abs/2510.07264}, 
      abbr = {arXiv preprint}
}

@article{angrisani2025classicallyestimatingobservablesnoiseless,
      title={Classically estimating observables of noiseless quantum circuits}, 
      author={Armando Angrisani and Alexander Schmidhuber and Manuel S. Rudolph and M. Cerezo and Zoë Holmes and Hsin-Yuan Huang},
      year={2025},
      eprint={2409.01706},
      archivePrefix={arXiv},
      primaryClass={quant-ph},
      url={https://arxiv.org/abs/2409.01706},
      abbr = {arXiv preprint} 
}


@article{rudolph2024trainabilitybarriers,
	abstract = {Quantum generative models provide inherently efficient sampling strategies and thus show promise for achieving an advantage using quantum hardware. In this work, we investigate the barriers to the trainability of quantum generative models posed by barren plateaus and exponential loss concentration. We explore the interplay between explicit and implicit models and losses, and show that using quantum generative models with explicit losses such as the KL divergence leads to a new flavor of barren plateaus. In contrast, the implicit Maximum Mean Discrepancy loss can be viewed as the expectation value of an observable that is either low-bodied and provably trainable, or global and untrainable depending on the choice of kernel. In parallel, we find that solely low-bodied implicit losses cannot in general distinguish high-order correlations in the target data, while some quantum loss estimation strategies can. We validate our findings by comparing different loss functions for modeling data from High-Energy-Physics.},
	author = {Rudolph, Manuel S. and Lerch, Sacha and Thanasilp, Supanut and Kiss, Oriel and Shaya, Oxana and Vallecorsa, Sofia and Grossi, Michele and Holmes, Zo{\"e}},
	date = {2024/11/13},
	date-added = {2025-11-05 17:09:30 +0100},
	date-modified = {2025-11-05 17:09:30 +0100},
	doi = {10.1038/s41534-024-00902-0},
	id = {Rudolph2024},
	isbn = {2056-6387},
	journal = {npj Quantum Information},
	number = {1},
	pages = {116},
	title = {Trainability barriers and opportunities in quantum generative modeling},
	url = {https://doi.org/10.1038/s41534-024-00902-0},
	volume = {10},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1038/s41534-024-00902-0},
  abbr = {npj}
  }


@article{eckstein2024largescalesimulations,
	abstract = {Periodically driven quantum systems exhibit a diverse set of phenomena but are more challenging to simulate than their equilibrium counterparts. Here, we introduce the Quantum High-Frequency Floquet Simulation (QHiFFS) algorithm as a method to simulate fast-driven quantum systems on quantum hardware. Central to QHiFFS is the concept of a kick operator which transforms the system into a basis where the dynamics is governed by a time-independent effective Hamiltonian. This allows prior methods for time-independent simulation to be lifted to simulate Floquet systems. We use the periodically driven biaxial next-nearest neighbor Ising (BNNNI) model, a natural test bed for quantum frustrated magnetism and criticality, as a case study to illustrate our algorithm. We implemented a 20-qubit simulation of the driven two-dimensional BNNNI model on Quantinuum's trapped ion quantum computer. Our error analysis shows that QHiFFS exhibits not only a cubic advantage in driving frequency ωbut also a linear advantage in simulation time t compared to Trotterization.},
	author = {Eckstein, Timo and Mansuroglu, Refik and Czarnik, Piotr and Zhu, Jian-Xin and Hartmann, Michael J. and Cincio, Lukasz and Sornborger, Andrew T. and Holmes, Zo{\"e}},
	date = {2024/09/13},
	date-added = {2025-11-05 17:10:01 +0100},
	date-modified = {2025-11-05 17:10:01 +0100},
	doi = {10.1038/s41534-024-00866-1},
	id = {Eckstein2024},
	isbn = {2056-6387},
	journal = {npj Quantum Information},
	number = {1},
	pages = {84},
	title = {Large-scale simulations of Floquet physics on near-term quantum computers},
	url = {https://doi.org/10.1038/s41534-024-00866-1},
	volume = {10},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1038/s41534-024-00866-1},
  abbr = {npj}
  }



@article{thanasilp2024exponentialconcentration,
	abstract = {Kernel methods in Quantum Machine Learning (QML) have recently gained significant attention as a potential candidate for achieving a quantum advantage in data analysis. Among other attractive properties, when training a kernel-based model one is guaranteed to find the optimal model's parameters due to the convexity of the training landscape. However, this is based on the assumption that the quantum kernel can be efficiently obtained from quantum hardware. In this work we study the performance of quantum kernel models from the perspective of the resources needed to accurately estimate kernel values. We show that, under certain conditions, values of quantum kernels over different input data can be exponentially concentrated (in the number of qubits) towards some fixed value. Thus on training with a polynomial number of measurements, one ends up with a trivial model where the predictions on unseen inputs are independent of the input data. We identify four sources that can lead to concentration including expressivity of data embedding, global measurements, entanglement and noise. For each source, an associated concentration bound of quantum kernels is analytically derived. Lastly, we show that when dealing with classical data, training a parametrized data embedding with a kernel alignment method is also susceptible to exponential concentration. Our results are verified through numerical simulations for several QML tasks. Altogether, we provide guidelines indicating that certain features should be avoided to ensure the efficient evaluation of quantum kernels and so the performance of quantum kernel methods.},
	author = {Thanasilp, Supanut and Wang, Samson and Cerezo, M. and Holmes, Zo{\"e}},
	date = {2024/06/18},
	date-added = {2025-11-05 17:10:43 +0100},
	date-modified = {2025-11-05 17:10:43 +0100},
	doi = {10.1038/s41467-024-49287-w},
	id = {Thanasilp2024},
	isbn = {2041-1723},
	journal = {Nature Communications},
	number = {1},
	pages = {5200},
	title = {Exponential concentration in quantum kernel methods},
	url = {https://doi.org/10.1038/s41467-024-49287-w},
	volume = {15},
	year = {2024},
	bdsk-url-1 = {https://doi.org/10.1038/s41467-024-49287-w},
  abbr = {Nat. Commun}
  }



